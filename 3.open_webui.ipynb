{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e53bca93-3209-4860-8108-317806038e9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T21:00:11.501436Z",
     "iopub.status.busy": "2025-03-04T21:00:11.500312Z",
     "iopub.status.idle": "2025-03-04T21:00:11.506385Z",
     "shell.execute_reply": "2025-03-04T21:00:11.504886Z",
     "shell.execute_reply.started": "2025-03-04T21:00:11.501389Z"
    }
   },
   "source": [
    "# Open WebUI 作为前端聊天框\n",
    "\n",
    "上一节我用 3 种方式部署了本地大模型：Ollama, vLLM 和 transformers.\n",
    "\n",
    "有了后端的推理服务，下一步该连接前端聊天框了。开源聊天框有很多，比如 AnythingLLM, LM Studio. 看了一圈，最推荐的还是 [Open WebUI](https://github.com/open-webui/open-webui). Open WebUI 有预开发的账号系统和历史对话模块，支持开发 RAG 和联网搜索。让我自己整出这么一个界面干净、布局合理的聊天框是不容易的。\n",
    "\n",
    "Open WebUI 的界面是这样的：\n",
    "\n",
    "![](./img/open_webui.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2efe74-78f1-486a-aabe-e235e25c16c1",
   "metadata": {},
   "source": [
    "## 1. 安装 Open WebUI\n",
    "\n",
    "Open WebUI 基本做到了开箱即用，但是部署它需要一些 Docker 基础。如果你对 [Docker](https://docs.docker.com/guides/) 不熟悉，可以参考我的两篇博客：\n",
    "\n",
    "- [Docker 命令速查手册](https://luochang212.github.io/posts/docker_command/)：介绍 Docker 常用命令\n",
    "- [Docker 部署 Spark 环境](https://luochang212.github.io/posts/synchrotrap/#%E9%99%84%E5%BD%951%E9%83%A8%E7%BD%B2-spark-%E7%8E%AF%E5%A2%83)：通过具体任务熟悉基础 Docker 操作 (GitHub: [SynchroTrap](https://github.com/luochang212/SynchroTrap/blob/main/%E9%99%84%E5%BD%951.%E9%83%A8%E7%BD%B2Spark%E7%8E%AF%E5%A2%83.ipynb))\n",
    "\n",
    "**1）检查 Docker 环境**\n",
    "\n",
    "确保已经安装 docker，且服务端已开启。如果使用 Windows，应该首先打开 Docker 桌面软件。\n",
    "\n",
    "打开命令行输入：\n",
    "\n",
    "```bash\n",
    "docker info\n",
    "```\n",
    "\n",
    "如果 Docker 是正常运行的，你应该看到一长串 INFO 信息，既包含 Server 信息，也包含 Client 信息。\n",
    "\n",
    "**2）下载镜像**\n",
    "\n",
    "这步最困难的地方在于，它需要一点网络魔法。目前国内似乎没有一个云服务商，免费提供 Docker 镜像的镜像站服务。\n",
    "\n",
    "```bash\n",
    "docker pull ghcr.io/open-webui/open-webui:cuda\n",
    "```\n",
    "\n",
    "**3）创建容器**\n",
    "\n",
    "下述命令使用镜像 `ghcr.io/open-webui/open-webui:cuda`，创建了一个容器。\n",
    "\n",
    "```bash\n",
    "docker run -d \\\n",
    "    --gpus all \\\n",
    "    -p 3215:8080 \\\n",
    "    -v ./data:/app/backend/data \\\n",
    "    --name open-webui \\\n",
    "    ghcr.io/open-webui/open-webui:cuda\n",
    "```\n",
    "\n",
    "参数解释：\n",
    "\n",
    "- `-d`：表示在后台运行容器\n",
    "- `--gpus all`：指定容器使用所有可用的 GPU 资源，一般用于多卡环境启用 GPU 支持\n",
    "- `-p 3215:8080`：将本机的 3215 端口映射到容器的 8080 端口，实现外部访问容器内服务\n",
    "- `-v ./data:/app/backend/data`：将本地 `./data` 路径挂载到容器的 `/app/backend/data` 路径，实现数据持久化\n",
    "- `--name`：指定容器名称为 `open-webui`\n",
    "\n",
    "⚠ 请注意：\n",
    "\n",
    "1. 由于我们将本地 `./data` 路径挂载到容器的 `/app/backend/data` 路径，请确保 `./data` 目录已被创建\n",
    "2. 由于我们将本机的 3215 端口映射到容器的 8080 端口，请确保本地的 3215 端口未被占用，如被占用需要更换端口\n",
    "\n",
    "**4）打开前端页面**\n",
    "\n",
    "浏览器打开 http://127.0.0.1:3215\n",
    "\n",
    "可能需要稍微加载一会儿，UI 界面才会出现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23acb595-ff97-4896-8415-30aed6c0016c",
   "metadata": {},
   "source": [
    "## 2. 为 WebUI 配置推理后端\n",
    "\n",
    "**1）Ollama**\n",
    "\n",
    "如果之前你已经运行 Ollama 模型，Ollama 会在默认端口 11434 提供服务。由于 Open WebUI 原生支持 Ollama，它会自动发现 11434 端口的 Ollama 服务。不需要做额外操作，就可以直接使用 Ollama 后端服务。\n",
    "\n",
    "如果还没有运行 Ollama，执行以下命令就可以：\n",
    "\n",
    "```bash\n",
    "ollama run deepseek-r1:1.5b\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b09d3-d147-4dc5-aa28-b4d004ccf6b2",
   "metadata": {},
   "source": [
    "**2）vLLM**\n",
    "\n",
    "Open WebUI 支持 OpenAI API 格式，也就是说只要你的 API 符合 OpenAI 的格式，就很容易接入 Open WebUI\n",
    ". 至于如何让自己的 API 符合 OpenAI 规范，可以参考这两个链接：\n",
    "\n",
    "- OpenAI 文档：[Create chat completion](https://platform.openai.com/docs/api-reference/chat/create)\n",
    "- OpenAI GitHub 仓库：[openai/openai-quickstart-python](https://github.com/openai/openai-quickstart-python)\n",
    "\n",
    "vLLM 支持从命令行直接启动 OpenAI 风格的 API. 如果你和我一样，不想花时间写 API，可以用以下代码快速启动。`--model` 后面跟本地模型的路径，也许你需要改一下。\n",
    "\n",
    "```bash\n",
    "# 1. 激活 conda 环境\n",
    "conda init && conda activate vllm_env\n",
    "\n",
    "# 2. 启动 vLLM OpenAI API 服务\n",
    "CUDA_VISIBLE_DEVICES=0 python3 -m vllm.entrypoints.openai.api_server \\\n",
    "    --model ./model/DeepSeek-R1-Distill-Qwen-1.5B \\\n",
    "    --served-model-name DeepSeek-R1-Distill-Qwen-1.5B \\\n",
    "    --gpu-memory-utilization 0.95 \\\n",
    "    --tensor-parallel-size 1 \\\n",
    "    --max-num-batched-tokens 51200 \\\n",
    "    --dtype auto \\\n",
    "    --enable-prefix-caching \\\n",
    "    --max-num-seqs 512 \\\n",
    "    --max-model-len 2048 \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 9494 \\\n",
    "    --api-key token-abc123456\n",
    "\n",
    "```\n",
    "\n",
    "vLLM 启动通常需要花一点时间，我们可以在这时候顺便配一下 Open WebUI 的设置。\n",
    "\n",
    "点左下角头像 -> Setting -> Admin Settings，然后进入 Connections 页面。把 OpenAI API 选项打开。再点击设置按钮进入 API 配置页。\n",
    "\n",
    "> 补充：这里还要把 Ollama API 临时关掉，否则优先使用 Ollama 服务。\n",
    "\n",
    "![](./img/open_webui_settings.jpg)\n",
    "\n",
    "配置页一共要填三行：\n",
    "\n",
    "- URL: http://host.docker.internal:9494/v1\n",
    "- Key: token-abc123456\n",
    "- Model IDs: DeepSeek-R1-Distill-Qwen-1.5B\n",
    "\n",
    "填完 Model ID 还要按 + 号把记录添加上去，再按 Save 保存，配置才会生效。\n",
    "\n",
    "![](./img/open_webui_add_model.jpg)\n",
    "\n",
    "填完以后，就可以在模型列表里看到 `DeepSeek-R1-Distill-Qwen-1.5B` 了。\n",
    "\n",
    "![](./img/open_webui_finish.jpg)\n",
    "\n",
    "用 bash 代码直接启动的 vLLM 服务并不靠谱，经常出现乱码、截断等等问题。还是第一节中的 vLLM 配置和启动方法靠谱。所以下一节，我们用 FastAPI + vLLM 开发一个符合 OpenAI 规范的模型后端 API，这样就能组合出一个可用的 vLLM 服务。\n",
    "\n",
    "可见当下 vLLM 生态与 DeepSeek 的兼容并不完美。如果只是想玩玩本地部署大模型 Ollama 足矣，折腾 vLLM 属实没必要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3798608f-53d7-46b7-9bd5-62b4e5f66d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
