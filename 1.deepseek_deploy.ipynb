{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7242912b-774f-4beb-8ae3-4c2ce422ec0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T08:48:10.403229Z",
     "iopub.status.busy": "2025-03-04T08:48:10.401145Z",
     "iopub.status.idle": "2025-03-04T08:48:10.410465Z",
     "shell.execute_reply": "2025-03-04T08:48:10.408455Z",
     "shell.execute_reply.started": "2025-03-04T08:48:10.402142Z"
    }
   },
   "source": [
    "# DeepSeek R1 模型部署指南：Ollama, vLLM 和 Transformers\n",
    "\n",
    "DeepSeek-R1 的爆火掀起一轮本地部署大模型的热潮，尤其是 DeepSeek-R1 的 [蒸馏模型](https://github.com/deepseek-ai/DeepSeek-R1#deepseek-r1-distill-models)，因参数量小、推理速度快而备受青睐。本着跑通流程、搭搭脚手架的心态，本次部署参数量最小的 `DeepSeek-R1-Distill-Qwen-1.5B`。\n",
    "\n",
    "大模型本地部署依赖推理引擎，目前比较流行的推理引擎有：\n",
    "\n",
    "|推理引擎|场景|介绍|\n",
    "| -- | -- |-- |\n",
    "|[Ollama](https://github.com/ollama/ollama)|适合个人开发者和轻量级应用|基于 [llama.cpp](https://github.com/ggml-org/llama.cpp) 开发，支持 CPU 推理，安装简单，开箱即用，适合快速原型开发和测试|\n",
    "|[vLLM](https://github.com/vllm-project/vllm)|适合高并发生产环境|支持多 GPU 并行、批处理、PagedAttention，吞吐量高，延迟低，适合大规模服务部署|\n",
    "|[Transformers](https://github.com/huggingface/transformers)|适合模型研究和实验|提供完整的模型训练和推理接口，支持模型微调、量化、加速，适合研究人员和需要深度定制的场景|\n",
    "|[SGLang](https://github.com/sgl-project/sglang)|适合需要复杂推理流程的场景|支持结构化输出、并行推理、流式输出，特别适合需要多轮对话和复杂推理的应用|\n",
    "|[LMDeploy](https://github.com/InternLM/lmdeploy)|适合企业级部署和边缘计算|由上海人工智能实验室开发，提供完整的模型量化、加速和部署工具链，支持多种硬件平台，特别适合资源受限场景|\n",
    "\n",
    "下面介绍如何部署 Ollama, vLLM, Transformers 这三款推理引擎，简要部署步骤见本项目 `/deploy` 目录。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ded3e-17da-4461-8c50-01c113829f15",
   "metadata": {},
   "source": [
    "## 1. Ollama\n",
    "\n",
    "Ollama 的部署非常简单，唯一考验的就是你打开命令行的能力 (*ﾟーﾟ)\n",
    "\n",
    "如果是 macOS 或者 Windows，从 [官网](https://ollama.com/download) 直接下载安装；如果是 Linux 系统，执行以下命令安装 Ollama：\n",
    "\n",
    "```bsah\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```\n",
    "\n",
    "验证 Ollama 是否安装成功：\n",
    "\n",
    "```bsah\n",
    "ollama --version\n",
    "```\n",
    "\n",
    "下载并运行 DeepSeek-R1-Distill-Qwen-1.5B 模型：\n",
    "\n",
    "```bsah\n",
    "ollama run deepseek-r1:1.5b\n",
    "```\n",
    "\n",
    "然后就可以在命令行使用 deepseek-r1 模型了。\n",
    "\n",
    "<!-- 如下图：\n",
    "\n",
    "![](./img/test_ollama.jpg)\n",
    "\n",
    "作为 `1.5b` 模型，能有如此效果，已是相当惊人了。 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28660b22-4181-4952-a433-56151c0998e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T18:01:46.326824Z",
     "iopub.status.busy": "2025-03-04T18:01:46.325822Z",
     "iopub.status.idle": "2025-03-04T18:01:46.362974Z",
     "shell.execute_reply": "2025-03-04T18:01:46.356310Z",
     "shell.execute_reply.started": "2025-03-04T18:01:46.326824Z"
    }
   },
   "source": [
    "## 2. vLLM\n",
    "\n",
    "vLLM 的部署也很简单，但是在个人电脑上的优化不如 Ollama，推荐有大显存 GPU 服务器的同学部署。\n",
    "\n",
    "建议在 Ubuntu 系统安装 vLLM。如果是 Windows 电脑，可以使用 `wsl`。\n",
    "\n",
    "我的系统配置如下：\n",
    "\n",
    "- **系统版本**：Ubuntu 24.04.1 LTS\n",
    "- **CUDA 版本**：12.6\n",
    "- **显卡**：RTX 4070\n",
    "- **显存**：8G\n",
    "\n",
    "在 Ubuntu 系统下，可以用以下命令查看系统配置：\n",
    "\n",
    "```bash\n",
    "# 查看系统版本\n",
    "uname -m && cat /etc/*release\n",
    "\n",
    "# 查看 CUDA 版本\n",
    "nvcc -V\n",
    "\n",
    "# 查看 NVIDIA 显卡信息\n",
    "nvidia-smi\n",
    "```\n",
    "\n",
    "下面进入正式的安装环节。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0de8a1-50c6-450d-9d17-3f8c9f161ad3",
   "metadata": {},
   "source": [
    "**1）安装 miniconda**\n",
    "\n",
    "参考：https://docs.anaconda.com/miniconda/install/#quick-command-line-install\n",
    "\n",
    "```bash\n",
    "mkdir -p ~/miniconda3\n",
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\n",
    "bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\n",
    "rm ~/miniconda3/miniconda.sh\n",
    "\n",
    "~/miniconda3/bin/conda init bash\n",
    "~/miniconda3/bin/conda init zsh\n",
    "```\n",
    "\n",
    "然后，你需要执行以下命令激活 conda 环境：\n",
    "\n",
    "```bash\n",
    "source ~/.bashrc\n",
    "```\n",
    "\n",
    "**2）安装 jupyterlab**\n",
    "\n",
    "```bash\n",
    "conda activate base\n",
    "conda install -c conda-forge jupyterlab\n",
    "```\n",
    "\n",
    "**3）为 vllm 创建虚拟环境，并绑定到 jupyterlab**\n",
    "\n",
    "```bash\n",
    "conda create --name vllm_env python=3.12\n",
    "conda activate vllm_env\n",
    "\n",
    "python -m pip install ipykernel\n",
    "python -m ipykernel install --user --name vllm_env --display-name \"Python (vllm_env)\"\n",
    "```\n",
    "\n",
    "**4）安装 vLLM 及相关依赖**\n",
    "\n",
    "```bash\n",
    "# 安装 uv\n",
    "pip install uv\n",
    "\n",
    "# 使用 uv 安装 vllm\n",
    "uv pip install vllm\n",
    "uv pip install ipywidgets transformers accelerate huggingface_hub\n",
    "\n",
    "# 验证安装是否成功\n",
    "vllm --version\n",
    "```\n",
    "\n",
    "<!-- 建议像我一样分多次安装。网好随意，网络不好还一起安装我怕你哭。\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 -i https://mirrors.aliyun.com/pypi/simple/\n",
    "pip install transformers -i https://mirrors.aliyun.com/pypi/simple/\n",
    "pip install accelerate huggingface_hub -i https://mirrors.aliyun.com/pypi/simple/\n",
    "\n",
    "pip install importlib_metadata -i https://mirrors.aliyun.com/pypi/simple/\n",
    "pip install vllm -i https://mirrors.aliyun.com/pypi/simple/\n",
    "pip install -U ipywidgets -i https://mirrors.aliyun.com/pypi/simple/\n",
    "``` -->\n",
    "\n",
    "**5）下载 DeepSeek-R1-Distill-Qwen-1.5B 模型**\n",
    "\n",
    "⚠ 注意：`--local-dir` 后面跟的是模型本地存储路径，请改成你的路径。\n",
    "\n",
    "```bash\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "huggingface-cli download --resume-download deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --local-dir ./model/DeepSeek-R1-Distill-Qwen-1.5B\n",
    "```\n",
    "\n",
    "> **Note:** 你也可以选择从 ModelScope 下载，下载方法见本仓库的 `/model/model_download_script/models_cope.py`. 但对于本教程来说不太建议，为了后续统一，请使用 huggingface 的方法下载。\n",
    "\n",
    "**6）启动 jupyterlab**\n",
    "\n",
    "```bash\n",
    "conda activate vllm_env\n",
    "jupyter lab --ip='0.0.0.0' --port=3234 --notebook-dir=/ --no-browser --allow-root\n",
    "```\n",
    "\n",
    "打开 http://localhost:3234/lab\n",
    "\n",
    "然后点开右上角按钮选择我们前面安装好的名为 `Python (vllm_env)` 的 Kernel\n",
    "\n",
    "![](./img/vllm_tutorial.jpg)\n",
    "\n",
    "**7）验证 vLLM**\n",
    "\n",
    "遵循 DeepSeek R1 的 [官方建议](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/README.md#usage-recommendations)：\n",
    "\n",
    "1. 温度设为 0.6\n",
    "2. 不写系统提示词\n",
    "3. 对于数学问题，追加提示词 \"Please reason step by step, and put your final answer within \\boxed{}.\"\n",
    "4. 为了确保触发思维链，在 prompt 里加 \"请以 \\<think\\>\\n 开头开始你的回答\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c47009-d1be-49ea-ab83-bba1c4fb8603",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T12:49:14.201370Z",
     "iopub.status.busy": "2025-03-20T12:49:14.201163Z",
     "iopub.status.idle": "2025-03-20T12:49:20.533509Z",
     "shell.execute_reply": "2025-03-20T12:49:20.532452Z",
     "shell.execute_reply.started": "2025-03-20T12:49:14.201358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-20 20:49:19 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import transformers\n",
    "import vllm\n",
    "\n",
    "# 指定使用哪一块显卡\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# 配置你的模型路径\n",
    "MODEL_PATH = './model/DeepSeek-R1-Distill-Qwen-1.5B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad518ea5-a72f-441f-b92b-176bd7a8928c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T12:49:20.534579Z",
     "iopub.status.busy": "2025-03-20T12:49:20.534417Z",
     "iopub.status.idle": "2025-03-20T12:49:20.539321Z",
     "shell.execute_reply": "2025-03-20T12:49:20.538079Z",
     "shell.execute_reply.started": "2025-03-20T12:49:20.534568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1+cu124\n",
      "torchvision version: 0.20.1+cu124\n"
     ]
    }
   ],
   "source": [
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d0296bf-606a-4a61-8c8f-f4d9b0a1168b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T12:49:20.539932Z",
     "iopub.status.busy": "2025-03-20T12:49:20.539789Z",
     "iopub.status.idle": "2025-03-20T12:53:10.550149Z",
     "shell.execute_reply": "2025-03-20T12:53:10.549188Z",
     "shell.execute_reply.started": "2025-03-20T12:49:20.539921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-20 20:49:25 config.py:542] This model supports multiple tasks: {'embed', 'score', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 03-20 20:49:25 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='./model/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='./model/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=./model/DeepSeek-R1-Distill-Qwen-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 03-20 20:49:26 interface.py:284] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 03-20 20:49:26 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 03-20 20:49:27 model_runner.py:1110] Starting to load model ./model/DeepSeek-R1-Distill-Qwen-1.5B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402c5fff42544a8d92daf54f396881c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-20 20:52:54 model_runner.py:1115] Loading model weights took 3.3460 GB\n",
      "INFO 03-20 20:52:59 worker.py:267] Memory profiling takes 5.30 seconds\n",
      "INFO 03-20 20:52:59 worker.py:267] the current vLLM instance can use total_gpu_memory (8.00GiB) x gpu_memory_utilization (0.95) = 7.60GiB\n",
      "INFO 03-20 20:52:59 worker.py:267] model weights take 3.35GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 3.16GiB; the rest of the memory reserved for KV Cache is 1.06GiB.\n",
      "INFO 03-20 20:52:59 executor_base.py:110] # CUDA blocks: 2474, # CPU blocks: 9362\n",
      "INFO 03-20 20:52:59 executor_base.py:115] Maximum concurrency for 2048 tokens per request: 19.33x\n",
      "INFO 03-20 20:53:00 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:10<00:00,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-20 20:53:10 model_runner.py:1562] Graph capturing finished in 10 secs, took 0.18 GiB\n",
      "INFO 03-20 20:53:10 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 16.11 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm = vllm.LLM(\n",
    "    model=MODEL_PATH,\n",
    "    gpu_memory_utilization=0.95,\n",
    "    max_model_len=2048,\n",
    "    tensor_parallel_size=1,\n",
    "    enable_prefix_caching=True,\n",
    "    max_num_batched_tokens=51200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9b15b09-f562-4163-a0b6-baa3f7764abf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T12:53:10.551388Z",
     "iopub.status.busy": "2025-03-20T12:53:10.551203Z",
     "iopub.status.idle": "2025-03-20T12:53:10.554880Z",
     "shell.execute_reply": "2025-03-20T12:53:10.553832Z",
     "shell.execute_reply.started": "2025-03-20T12:53:10.551376Z"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    '计算数学问题 (1 + 3) ^ 2 = ? 请逐步进行推理，并将你的最终答案放在 \\\\boxed{} 内。',\n",
    "    '你将扮演一个内心火热但是表面冷淡的小偶像，请用暗含深切热爱的态度，对粉丝的晚安动态进行回复。'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8678a3f-6e17-4594-9b00-3a1353ec842b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T12:53:10.557495Z",
     "iopub.status.busy": "2025-03-20T12:53:10.557168Z",
     "iopub.status.idle": "2025-03-20T12:53:12.446556Z",
     "shell.execute_reply": "2025-03-20T12:53:12.445397Z",
     "shell.execute_reply.started": "2025-03-20T12:53:10.557479Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.07it/s, est. speed input: 33.22 toks/s, output: 75.01 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义采样参数\n",
    "sampling_params = vllm.SamplingParams(temperature=0.6,\n",
    "                                      top_p=0.95,\n",
    "                                      max_tokens=8192,\n",
    "                                      stop_token_ids=[151329, 151336, 151338])\n",
    "\n",
    "# 模型推理\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9386d3e4-fe1c-435c-94ad-38a36458e54c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T12:53:12.447478Z",
     "iopub.status.busy": "2025-03-20T12:53:12.447217Z",
     "iopub.status.idle": "2025-03-20T12:53:12.455548Z",
     "shell.execute_reply": "2025-03-20T12:53:12.453438Z",
     "shell.execute_reply.started": "2025-03-20T12:53:12.447456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "计算数学问题 (1 + 3) ^ 2 = ? 请逐步进行推理，并将你的最终答案放在 \\boxed{} 内。\n",
      "Generated text:\n",
      "<think>\n",
      "]\n",
      "\n",
      "解题思路：\n",
      "首先，计算括号内的内容，即1加3等于4。然后，对结果4进行平方运算，即4的平方等于16。因此，最终的答案是16。\n",
      "</think>\n",
      "\n",
      "首先，计算括号内的内容：\n",
      "\n",
      "\\[\n",
      "1 + 3 = 4\n",
      "\\]\n",
      "\n",
      "然后，对结果进行平方运算：\n",
      "\n",
      "\\[\n",
      "4^2 = 16\n",
      "\\]\n",
      "\n",
      "因此，最终的答案是：\n",
      "\n",
      "\\[\n",
      "\\boxed{16}\n",
      "\\]\n",
      "\n",
      "\n",
      "Prompt:\n",
      "你将扮演一个内心火热但是表面冷淡的小偶像，请用暗含深切热爱的态度，对粉丝的晚安动态进行回复。\n",
      "Generated text:\n",
      "<think>\n",
      "请回复一到两句，语气要温暖，让人感到安心，不要让粉丝感到被吸引或被感染。\n",
      "</think>\n",
      "\n",
      "晚安！希望你一切安好！\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt:\\n{prompt}\")\n",
    "    print(f\"Generated text:\\n<think>\\n{generated_text}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc38bea-0bbd-42a1-9155-a5c580f7b479",
   "metadata": {},
   "source": [
    "## 3. Transformers\n",
    "\n",
    "transformers 是 HuggingFace 的开源项目，它支持大量 Transformer 架构模型的推理，包括 deepseek。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7c44c1e-9e9f-4709-88a3-be99af56396d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T12:54:26.989717Z",
     "iopub.status.busy": "2025-03-20T12:54:26.989343Z",
     "iopub.status.idle": "2025-03-20T12:54:28.333533Z",
     "shell.execute_reply": "2025-03-20T12:54:28.332302Z",
     "shell.execute_reply.started": "2025-03-20T12:54:26.989692Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# 指定使用哪一块显卡\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# 配置你的模型路径\n",
    "MODEL_PATH = './model/DeepSeek-R1-Distill-Qwen-1.5B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0128f06-c3ba-4bef-a640-d0ec5796b2df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T12:54:28.336673Z",
     "iopub.status.busy": "2025-03-20T12:54:28.335895Z",
     "iopub.status.idle": "2025-03-20T12:54:28.344520Z",
     "shell.execute_reply": "2025-03-20T12:54:28.342867Z",
     "shell.execute_reply.started": "2025-03-20T12:54:28.336637Z"
    }
   },
   "outputs": [],
   "source": [
    "# 文本分割函数\n",
    "def split_text(text):\n",
    "    # 定义正则表达式模式\n",
    "    pattern = re.compile(r'<think>(.*?)</think>(.*)', re.DOTALL)\n",
    "    # pattern = re.compile(r'(<think>)?(.*?)</think>(.*)', re.DOTALL)\n",
    "    match = pattern.search(text) # 匹配 <think>思考过程</think>回答\n",
    "\n",
    "    if match: # 如果匹配到思考过程\n",
    "        think_content = match.group(1) if match.group(1) is not None else \"\"\n",
    "        think_content = think_content.strip() # 获取思考过程\n",
    "        answer_content = match.group(2).strip() # 获取回答\n",
    "    else:\n",
    "        think_content = \"\" # 如果没有匹配到思考过程，则设置为空字符串\n",
    "        answer_content = text.strip() # 直接返回回答\n",
    "\n",
    "    return think_content, answer_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd71161e-9647-4efc-841d-e60c4b99b9a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T12:54:28.346013Z",
     "iopub.status.busy": "2025-03-20T12:54:28.345698Z",
     "iopub.status.idle": "2025-03-20T12:58:00.746972Z",
     "shell.execute_reply": "2025-03-20T12:58:00.746443Z",
     "shell.execute_reply.started": "2025-03-20T12:54:28.345987Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=False)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(MODEL_PATH,\n",
    "                                                          device_map='auto',\n",
    "                                                          torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c761b447-007a-40d2-aa9d-cf6053e51479",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T12:58:00.748363Z",
     "iopub.status.busy": "2025-03-20T12:58:00.748010Z",
     "iopub.status.idle": "2025-03-20T12:58:00.751054Z",
     "shell.execute_reply": "2025-03-20T12:58:00.750425Z",
     "shell.execute_reply.started": "2025-03-20T12:58:00.748348Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = '你将扮演一个内心火热但是表面冷淡的小偶像，请用暗含深切热爱的态度，对粉丝的晚安动态进行回复。'\n",
    "messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "437baeed-0308-4473-8ac2-14777c854bf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T12:58:00.751857Z",
     "iopub.status.busy": "2025-03-20T12:58:00.751573Z",
     "iopub.status.idle": "2025-03-20T12:58:08.349013Z",
     "shell.execute_reply": "2025-03-20T12:58:08.348095Z",
     "shell.execute_reply.started": "2025-03-20T12:58:00.751843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "think: 好的，用户让我扮演一个内心火热但表面冷淡的小偶像，用暗含深切热爱的态度回复粉丝的晚安动态。首先，我需要理解用户的需求，他们希望回复既温暖又有情感，同时保持冷淡的一面。\n",
      "\n",
      "用户可能希望回复显得真诚，但又不显太浓烈。所以，我应该用细腻的描写来传达情感，同时保持冷淡的语气。比如，用“温柔”来形容粉丝的感受，而“冷淡”则是语气上的选择。\n",
      "\n",
      "接下来，我需要考虑如何在回复中表现出理解和关怀，同时又不显得过于情绪化。或许可以提到粉丝的陪伴，强调他们的重要性，但又不直接表达情感。\n",
      "\n",
      "另外，用户可能希望回复带有温暖的感觉，所以可以用一些温暖的词汇，比如“温暖”、“陪伴”等，来传达情感。\n",
      "\n",
      "最后，我应该确保回复结构清晰，情感真挚，同时保持冷淡的语气，让粉丝感受到被理解和支持。\n",
      "answer: 亲爱的粉丝，你的陪伴是我最大的幸福，你总是在我最需要的时候站在我身边。你是我最忠实的依赖，也给了我无尽的温暖与关怀。你是我生命中最珍贵的财富，不会让我失望。希望你每天都能感受到我的爱与支持，永远支持我，共同书写属于我们的故事。\n"
     ]
    }
   ],
   "source": [
    "# 应用对话模型\n",
    "input_ids = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "model_inputs = tokenizer([input_ids], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 文本生成\n",
    "generated_ids = model.generate(model_inputs.input_ids,\n",
    "                               attention_mask=model_inputs.attention_mask,\n",
    "                               pad_token_id=tokenizer.eos_token_id,\n",
    "                               temperature=0.6,\n",
    "                               max_new_tokens=8192,  # 思考需要的 Token 数，设为 8K\n",
    "                               top_p=0.95)\n",
    "\n",
    "# 生成结果后处理：通过切片剔除输入部分，仅保留模型生成的内容\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "# 解码：将 token id 转换为自然语言文本，并跳过特殊标记\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response = '<think>\\n' + response\n",
    "think_content, answer_content = split_text(response)\n",
    "\n",
    "print(f\"think: {think_content}\")\n",
    "print(f\"answer: {answer_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb08b4-99e3-408f-b1a9-bf0c8f7ef2fa",
   "metadata": {},
   "source": [
    "参考：\n",
    "\n",
    "- vLLM GitHub: [vllm-project/vllm](https://github.com/vllm-project/vllm)\n",
    "- vLLM Docs: [docs.vllm.ai](https://docs.vllm.ai/en/latest/features/reasoning_outputs.html)\n",
    "- self-llm GitHub: [datawhalechina/self-llm](https://github.com/datawhalechina/self-llm/tree/master/models/DeepSeek-R1-Distill-Qwen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f198ea7-aed1-41af-aacc-3374af22cef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
