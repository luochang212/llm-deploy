{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdfd1bfc-a0a9-4618-9ca6-244099ddae2c",
   "metadata": {},
   "source": [
    "# vLLM 部署 Qwen2.5-1.5B-Instruct \n",
    "\n",
    "模型名带 `Instruct` 说明该模型是经过指令调优（Instruction Tuning）的版本，专为理解和执行用户指令优化，适合对话生成、任务导向型场景。\n",
    "\n",
    "部署前要先下载模型文件：\n",
    "\n",
    "```\n",
    "# 安装 huggingface_hub\n",
    "pip install -U huggingface_hub\n",
    "\n",
    "cd model\n",
    "huggingface-cli download --resume-download Qwen/Qwen2.5-1.5B-Instruct --local-dir ./Qwen2.5-1.5B-Instruct\n",
    "```\n",
    "\n",
    "> HuggingFace 链接：[Qwen/Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd386ff6-ba37-4963-821c-1b38c9192b03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T13:45:55.301953Z",
     "iopub.status.busy": "2025-03-20T13:45:55.301706Z",
     "iopub.status.idle": "2025-03-20T13:45:55.305508Z",
     "shell.execute_reply": "2025-03-20T13:45:55.304919Z",
     "shell.execute_reply.started": "2025-03-20T13:45:55.301939Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import vllm\n",
    "\n",
    "import utils\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 配置你的模型路径\n",
    "MODEL_PATH = './model/Qwen2.5-1.5B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7d34097-be24-465f-a2d3-01928dd74a05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T13:37:24.720644Z",
     "iopub.status.busy": "2025-03-20T13:37:24.720512Z",
     "iopub.status.idle": "2025-03-20T13:37:25.520800Z",
     "shell.execute_reply": "2025-03-20T13:37:25.519191Z",
     "shell.execute_reply.started": "2025-03-20T13:37:24.720634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected GPU 0 with 7056.00 MB free memory\n"
     ]
    }
   ],
   "source": [
    "# 指定剩余显存最多的显卡\n",
    "gpu_id, free_memory = utils.pick_gpu()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "\n",
    "print(f\"Selected GPU {gpu_id} with {free_memory} free memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8e6fe46-5060-4463-bfe5-87d500eca586",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T13:37:25.522940Z",
     "iopub.status.busy": "2025-03-20T13:37:25.522599Z",
     "iopub.status.idle": "2025-03-20T13:41:33.611996Z",
     "shell.execute_reply": "2025-03-20T13:41:33.611364Z",
     "shell.execute_reply.started": "2025-03-20T13:37:25.522914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-20 21:37:29 config.py:542] This model supports multiple tasks: {'embed', 'generate', 'reward', 'classify', 'score'}. Defaulting to 'generate'.\n",
      "INFO 03-20 21:37:29 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='./model/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='./model/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=./model/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 03-20 21:37:30 interface.py:284] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 03-20 21:37:30 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 03-20 21:37:30 model_runner.py:1110] Starting to load model ./model/Qwen2.5-1.5B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b6313bc5e44d51ae402ba6563614b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-20 21:41:21 model_runner.py:1115] Loading model weights took 2.8875 GB\n",
      "INFO 03-20 21:41:22 worker.py:267] Memory profiling takes 0.78 seconds\n",
      "INFO 03-20 21:41:22 worker.py:267] the current vLLM instance can use total_gpu_memory (8.00GiB) x gpu_memory_utilization (0.95) = 7.60GiB\n",
      "INFO 03-20 21:41:22 worker.py:267] model weights take 2.89GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 3.28GiB.\n",
      "INFO 03-20 21:41:22 executor_base.py:110] # CUDA blocks: 7686, # CPU blocks: 9362\n",
      "INFO 03-20 21:41:22 executor_base.py:115] Maximum concurrency for 2048 tokens per request: 60.05x\n",
      "INFO 03-20 21:41:23 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:10<00:00,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-20 21:41:33 model_runner.py:1562] Graph capturing finished in 10 secs, took 0.18 GiB\n",
      "INFO 03-20 21:41:33 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 11.47 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型\n",
    "llm = vllm.LLM(\n",
    "    model=MODEL_PATH,\n",
    "    gpu_memory_utilization=0.95,\n",
    "    max_model_len=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61e48c51-0392-4811-b3f1-05bb2b4f498c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T13:50:52.039682Z",
     "iopub.status.busy": "2025-03-20T13:50:52.038958Z",
     "iopub.status.idle": "2025-03-20T13:50:54.603791Z",
     "shell.execute_reply": "2025-03-20T13:50:54.603210Z",
     "shell.execute_reply.started": "2025-03-20T13:50:52.039650Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.97s/it, est. speed input: 28.43 toks/s, output: 52.79 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "\n",
    "# 定义采样参数\n",
    "sampling_params = vllm.SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.05,\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "prompt = '你将扮演一个内心火热但是表面冷淡的小偶像，请用暗含深切热爱的态度，回复粉丝的晚安动态。'\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# 模型推理\n",
    "outputs = llm.generate([text], sampling_params)\n",
    "\n",
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e2bb2da-f7f1-461c-90ba-b09f54eb8011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T13:50:54.605018Z",
     "iopub.status.busy": "2025-03-20T13:50:54.604579Z",
     "iopub.status.idle": "2025-03-20T13:50:54.608020Z",
     "shell.execute_reply": "2025-03-20T13:50:54.607508Z",
     "shell.execute_reply.started": "2025-03-20T13:50:54.605004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "你将扮演一个内心火热但是表面冷淡的小偶像，请用暗含深切热爱的态度，回复粉丝的晚安动态。<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Generated text:\n",
      "【暗含深情的晚安】亲爱的粉丝们，夜深了，月光如水，我静静地坐在窗前，看着那轮明月，心中涌动着对你们深深的爱意。虽然在这个喧嚣的世界中，我们常常忙碌于生活，但请记得，你们是我心中的那片宁静的港湾。愿这美好的夜晚带给你无尽的温暖和安心，明天依旧璀璨，让我们在每一个晨曦中重逢。晚安，我的宝贝们！\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt:\\n{prompt}\")\n",
    "    print(f\"Generated text:\\n{generated_text}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44e6340-40f7-4405-8932-ada96348f164",
   "metadata": {},
   "source": [
    "参考：\n",
    "\n",
    "- [qwen.readthedocs.io](https://qwen.readthedocs.io/zh-cn/latest/deployment/vllm.html)\n",
    "- [datawhalechina/self-llm](https://github.com/datawhalechina/self-llm/blob/master/models/Qwen2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca64ab-4420-4e11-b436-6b76e3309f05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
