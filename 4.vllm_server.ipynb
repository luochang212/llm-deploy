{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae62e4d-20e2-4639-9160-404347f8c322",
   "metadata": {},
   "source": [
    "# vLLM 推理服务\n",
    "\n",
    "vLLM 官方支持一行 bash 命令，启动符合 OpenAI 的接口协议要求的 API 服务：\n",
    "\n",
    "```bash\n",
    "vllm serve [YOUR MODEL or MODEL PATH]\n",
    "```\n",
    "\n",
    "但是对不同的模型，启动参数也略有不同。比如对于 `deepseek-r1` 这种推理模型，必须添加 `--enable-reasoning` 和 `--reasoning-parser deepseek_r1` 参数。具体每个模型如何使用 vLLM，可以参考 vLLM 官方文档或模型文档。\n",
    "\n",
    "参考资料：\n",
    "\n",
    "- [vLLM 官方文档](https://docs.vllm.ai/en/latest/index.html)\n",
    "- [OpenAI 接口文档](https://platform.openai.com/docs/api-reference/chat/create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e2bff16-d585-442d-aca2-65f293bd47c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T09:06:44.065095Z",
     "iopub.status.busy": "2025-03-21T09:06:44.064826Z",
     "iopub.status.idle": "2025-03-21T09:06:44.068949Z",
     "shell.execute_reply": "2025-03-21T09:06:44.068181Z",
     "shell.execute_reply.started": "2025-03-21T09:06:44.065077Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172f3345-8b50-4f86-a795-ebcdc3fe1110",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T09:06:44.069947Z",
     "iopub.status.busy": "2025-03-21T09:06:44.069733Z",
     "iopub.status.idle": "2025-03-21T09:06:44.353480Z",
     "shell.execute_reply": "2025-03-21T09:06:44.353068Z",
     "shell.execute_reply.started": "2025-03-21T09:06:44.069931Z"
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96b5265-6c60-49a3-b73d-1955017433e4",
   "metadata": {},
   "source": [
    "## 1. DeepSeek-R1\n",
    "\n",
    "**1）启动服务端**\n",
    "\n",
    "我预先在 `/server` 写了 vLLM 的 deepseek-r1 启动脚本。可以直接到文件夹下执行：\n",
    "\n",
    "```bash\n",
    "cd server\n",
    "bash ds_vllm_bash_server.sh\n",
    "```\n",
    "\n",
    "或者直接在命令行中执行：\n",
    "\n",
    "```bash\n",
    "conda activate vllm_env && \\\n",
    "    CUDA_VISIBLE_DEVICES=0 vllm serve \"./model/DeepSeek-R1-Distill-Qwen-1.5B\" \\\n",
    "        --served-model-name deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \\\n",
    "        --enable-reasoning \\\n",
    "        --reasoning-parser deepseek_r1 \\\n",
    "        --host 0.0.0.0 \\\n",
    "        --port 8000 \\\n",
    "        --gpu-memory-utilization 0.95 \\\n",
    "        --max-seq-len-to-capture 8192 \\\n",
    "        --tensor-parallel-size 1 \\\n",
    "        --api-key token-abc123456\n",
    "```\n",
    "\n",
    "**2）运行客户端**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f022112-f20b-4780-9e83-5dcae0d22c34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T09:06:44.354492Z",
     "iopub.status.busy": "2025-03-21T09:06:44.354201Z",
     "iopub.status.idle": "2025-03-21T09:06:44.358685Z",
     "shell.execute_reply": "2025-03-21T09:06:44.357572Z",
     "shell.execute_reply.started": "2025-03-21T09:06:44.354473Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# USAGE: python3 ds_vllm_bash_client.py\n",
    "# https://docs.vllm.ai/en/latest/features/reasoning_outputs.html\n",
    "# pip install vllm --upgrade\n",
    "\n",
    "\n",
    "openai_api_key = \"token-abc123456\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "\n",
    "def chat_completion(prompt, model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"):\n",
    "    client = OpenAI(\n",
    "        api_key=openai_api_key,\n",
    "        base_url=openai_api_base,\n",
    "    )\n",
    "\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.6,\n",
    "        top_p=0.95,\n",
    "        max_tokens=512,\n",
    "        extra_body={\n",
    "            \"repetition_penalty\": 1.05,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "920db724-d121-4ad0-af7d-a43d872476b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T09:06:44.359192Z",
     "iopub.status.busy": "2025-03-21T09:06:44.359064Z",
     "iopub.status.idle": "2025-03-21T09:06:44.364943Z",
     "shell.execute_reply": "2025-03-21T09:06:44.364333Z",
     "shell.execute_reply.started": "2025-03-21T09:06:44.359183Z"
    }
   },
   "outputs": [],
   "source": [
    "# response = chat_completion(prompt=\"什么是大模型？\")\n",
    "# content = response.choices[0].message.content\n",
    "# print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465966c2-8d1b-48e0-af56-b54c1f34b6ad",
   "metadata": {},
   "source": [
    "## 2. Qwen\n",
    "\n",
    "**1）启动服务端**\n",
    "\n",
    "使用仓库中的 bash 脚本：\n",
    "\n",
    "```bash\n",
    "cd server\n",
    "bash qwen_vllm_bash_server.sh\n",
    "```\n",
    "\n",
    "或者直接在命令行中执行：\n",
    "\n",
    "```bash\n",
    "conda activate vllm_env && \\\n",
    "    vllm serve \"./model/Qwen2.5-1.5B-Instruct/\" \\\n",
    "        --served-model-name Qwen/Qwen2.5-7B-Instruct \\\n",
    "        --host 0.0.0.0 \\\n",
    "        --port 8000 \\\n",
    "        --gpu-memory-utilization 0.98 \\\n",
    "        --tensor-parallel-size 1 \\\n",
    "        --api-key token-kcgyrk\n",
    "```\n",
    "\n",
    "**2）运行客户端**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c56a43a7-7bc2-466f-a397-525c1d2f4f7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T09:06:44.365298Z",
     "iopub.status.busy": "2025-03-21T09:06:44.365221Z",
     "iopub.status.idle": "2025-03-21T09:06:44.371125Z",
     "shell.execute_reply": "2025-03-21T09:06:44.370378Z",
     "shell.execute_reply.started": "2025-03-21T09:06:44.365290Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# USAGE: python3 qwen_vllm_bash_client.py\n",
    "\n",
    "\n",
    "openai_api_key = \"token-kcgyrk\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "\n",
    "def chat_completion(prompt, model=\"Qwen/Qwen2.5-7B-Instruct\"):\n",
    "    client = OpenAI(\n",
    "        api_key=openai_api_key,\n",
    "        base_url=openai_api_base,\n",
    "    )\n",
    "\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.8,\n",
    "        top_p=0.9,\n",
    "        max_tokens=512,\n",
    "        extra_body={\n",
    "            \"repetition_penalty\": 1.05,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a10fdbde-c06e-46ee-bca7-60953bfc24d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T09:06:44.372241Z",
     "iopub.status.busy": "2025-03-21T09:06:44.371728Z",
     "iopub.status.idle": "2025-03-21T09:06:44.376213Z",
     "shell.execute_reply": "2025-03-21T09:06:44.375733Z",
     "shell.execute_reply.started": "2025-03-21T09:06:44.372228Z"
    }
   },
   "outputs": [],
   "source": [
    "# response = chat_completion(prompt=\"什么是大模型？\")\n",
    "# content = response.choices[0].message.content\n",
    "# print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86675ba6-195f-4b1e-9d33-602c087e563f",
   "metadata": {},
   "source": [
    "## 3. 符合 OpenAI 接口协议的 API 服务\n",
    "\n",
    "我们可以不依赖 vLLM 的命令行，而是按照 OpenAI 的接口文档，自行开发一个符合 OpenAI 接口规范的 API。这样可以获得更多自定义的权力，比如可以按自己方式实现预处理和后处理步骤。\n",
    "\n",
    "以 `deepseek-r1` 模型服务为例，我用 FastAPI 实现了一个支持 `openai` 库调用的 API Server，但只实现两个核心路由：\n",
    "\n",
    "- `/v1/models`: 查看当前服务支持的模型列表\n",
    "- `/v1/chat/completions`: 传入用户问题，调用模型获取回答\n",
    "\n",
    "以下代码同本仓库的 `/server/ds_vllm_server.py` 代码文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c37d5ed-2749-4c84-942c-5d8fc9aeeb27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T09:06:44.376672Z",
     "iopub.status.busy": "2025-03-21T09:06:44.376561Z",
     "iopub.status.idle": "2025-03-21T09:06:47.687055Z",
     "shell.execute_reply": "2025-03-21T09:06:47.686517Z",
     "shell.execute_reply.started": "2025-03-21T09:06:44.376663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-21 17:06:47 [__init__.py:256] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# DESC: vLLM openai server\n",
    "# REFS:\n",
    "#   - https://platform.openai.com/docs/api-reference/chat/create\n",
    "#   - https://github.com/openai/openai-quickstart-python\n",
    "#   - https://fastapi.tiangolo.com/advanced/events/\n",
    "# USAGE: \n",
    "#   conda activate vllm_env\n",
    "#   python3 ds_vllm_server.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import vllm\n",
    "import time\n",
    "import uuid\n",
    "import uvicorn\n",
    "\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "from fastapi import FastAPI, HTTPException, Depends\n",
    "from fastapi.security import HTTPBearer\n",
    "from fastapi.responses import StreamingResponse\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "\n",
    "# 配置 API 密钥\n",
    "API_KEY = \"token-abc123456\"\n",
    "MODEL_NAME = \"DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "MODEL_PATH = \"../model/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "\n",
    "# 指定使用哪一块显卡\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "llm_model = {}\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    llm = vllm.LLM(\n",
    "        model=MODEL_PATH,\n",
    "        gpu_memory_utilization=0.95,\n",
    "        max_model_len=4096,\n",
    "        tensor_parallel_size=1,\n",
    "        enable_prefix_caching=True,\n",
    "        max_num_batched_tokens=51200\n",
    "    )\n",
    "\n",
    "    return llm\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    llm = load_model()\n",
    "    llm_model[MODEL_NAME] = llm\n",
    "    yield\n",
    "    llm_model.clear()\n",
    "\n",
    "\n",
    "app = FastAPI(lifespan=lifespan)\n",
    "security = HTTPBearer()\n",
    "\n",
    "\n",
    "class ChatMessage(BaseModel):\n",
    "    role: str  # \"user\", \"assistant\", \"system\"\n",
    "    content: str\n",
    "\n",
    "\n",
    "class ChatCompletionRequest(BaseModel):\n",
    "    model: str\n",
    "    messages: List[ChatMessage]\n",
    "    max_tokens: Optional[int] = 8192\n",
    "    temperature: Optional[float] = 0.6\n",
    "    top_p: Optional[float] = 0.95\n",
    "    n: Optional[int] = 1\n",
    "    stream: Optional[bool] = False\n",
    "    stop: Optional[List[str]] = None\n",
    "    presence_penalty: Optional[float] = 0.0\n",
    "    frequency_penalty: Optional[float] = 0.0\n",
    "\n",
    "\n",
    "def verify_token(credentials: HTTPBearer = Depends(security)):\n",
    "    if credentials.credentials != API_KEY:\n",
    "        raise HTTPException(401, \"Invalid API Key\")\n",
    "\n",
    "\n",
    "def split_text(text):\n",
    "    \"\"\"文本分割函数\"\"\"\n",
    "    pattern = re.compile(r'<think>(.*?)</think>(.*)', re.DOTALL)\n",
    "    match = pattern.search(text) # 匹配思考过程\n",
    "\n",
    "    if match: # 如果匹配到思考过程\n",
    "        think_content = match.group(1) if match.group(1) is not None else \"\"\n",
    "        think_content = think_content.strip()\n",
    "        answer_content = match.group(2).strip()\n",
    "    else:\n",
    "        think_content = \"\"\n",
    "        answer_content = text.strip()\n",
    "\n",
    "    return think_content, answer_content\n",
    "\n",
    "\n",
    "def model_infr(message: str,\n",
    "               model,\n",
    "               temperature=0.6,\n",
    "               max_tokens=8192,\n",
    "               top_p=0.95,\n",
    "               stop_token_ids=[151329, 151336, 151338]):\n",
    "\n",
    "    # 定义采样参数\n",
    "    sampling_params = vllm.SamplingParams(temperature=temperature,\n",
    "                                          top_p=top_p,\n",
    "                                          max_tokens=max_tokens,\n",
    "                                          stop_token_ids=stop_token_ids)\n",
    "    # stop_token_ids or [model.llm_engine.tokenizer.eos_token_id]\n",
    "\n",
    "    # 应用对话模型\n",
    "    output = model.generate(message, sampling_params)\n",
    "    response = output[0].outputs[0].text\n",
    "    response = f'<think>\\n{response}'\n",
    "    think_content, answer_content = split_text(response)\n",
    "\n",
    "    return think_content, answer_content\n",
    "\n",
    "\n",
    "def format_prompt(messages) -> str:\n",
    "    \"\"\"仅保留最后一轮 user 的对话\"\"\"\n",
    "    # 倒序遍历找到最后一个用户消息\n",
    "    for message in reversed(messages):\n",
    "        if message.role == \"user\":\n",
    "            return message.content  # 直接返回字符串内容\n",
    "    return \"\"  # 没有用户消息时返回空字符串\n",
    "\n",
    "\n",
    "@app.get(\"/v1/models\", dependencies=[Depends(verify_token)])\n",
    "async def list_models():\n",
    "    return {\n",
    "        \"object\": \"list\",\n",
    "        \"data\": [\n",
    "            {\n",
    "                \"id\": MODEL_NAME,\n",
    "                \"object\": \"model\",\n",
    "                \"created\": int(time.time()),\n",
    "                \"owned_by\": \"user\",\n",
    "                \"permissions\": []\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "@app.post(\"/v1/chat/completions\", dependencies=[Depends(verify_token)])\n",
    "async def create_chat_completion(request: ChatCompletionRequest):\n",
    "    # 校验\n",
    "    model_name = request.model\n",
    "    models = [MODEL_NAME]\n",
    "    if model_name not in models:\n",
    "        raise HTTPException(400, f\"{model_name} not in {models}\")\n",
    "\n",
    "    if request.n > 1 and not request.stream:\n",
    "        raise HTTPException(400, \"Only n=1 supported in non-streaming mode\")\n",
    "    if request.temperature is not None and request.temperature < 0:\n",
    "        raise HTTPException(400, \"Temperature must be ≥ 0\")\n",
    "    if request.top_p is not None and (request.top_p < 0 or request.top_p > 1):\n",
    "        raise HTTPException(400, \"Top_p must be between 0 and 1\")\n",
    "\n",
    "    # 模型推理\n",
    "    prompt = format_prompt(request.messages)\n",
    "    think_content, answer_content = model_infr(\n",
    "        message=prompt,\n",
    "        model=llm_model[model_name],\n",
    "        temperature=request.temperature,\n",
    "        max_tokens=request.max_tokens,\n",
    "        top_p=request.top_p\n",
    "    )\n",
    "\n",
    "    # 输出排版\n",
    "    lst = [\n",
    "        \"<think>\",\n",
    "        think_content,\n",
    "        \"</think>\",\n",
    "        answer_content\n",
    "    ]\n",
    "    content = \"\\n\".join(lst)\n",
    "\n",
    "    return {\n",
    "        \"id\": f\"chatcmpl-{str(uuid.uuid4())}\",\n",
    "        \"object\": \"chat.completion\",\n",
    "        \"created\": int(time.time()),\n",
    "        \"model\": model_name,\n",
    "        \"choices\": [{\n",
    "            \"index\": 0,\n",
    "            \"message\": {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": content.strip()\n",
    "            },\n",
    "            \"finish_reason\": \"stop\"\n",
    "        }]\n",
    "    }\n",
    "\n",
    "\n",
    "# uvicorn.run(app, host=\"0.0.0.0\", port=9494, log_level=\"debug\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af69e68-d755-4da9-9bb0-bd7ac717d614",
   "metadata": {},
   "source": [
    "## 4. 强制清理显存缓存\n",
    "\n",
    "如果发现显存不够，可以挣扎一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a71d1a77-a2be-41d3-9c40-9f2bb08b1374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T09:06:47.687999Z",
     "iopub.status.busy": "2025-03-21T09:06:47.687874Z",
     "iopub.status.idle": "2025-03-21T09:06:48.075964Z",
     "shell.execute_reply": "2025-03-21T09:06:48.074344Z",
     "shell.execute_reply.started": "2025-03-21T09:06:47.687989Z"
    }
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "utils.torch_gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbb036b-1255-430f-8de6-62f6699769cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vllm_env)",
   "language": "python",
   "name": "vllm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
